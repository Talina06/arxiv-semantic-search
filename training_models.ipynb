{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrRUcypob-Lu"
      },
      "source": [
        "# Model Training on Academic Dataset\n",
        "\n",
        "In this notebook, we are using 2 base models and training them on arxiv research dataset to be able to perform efficient summarization and semantic search.\n",
        "\n",
        "## Summarization\n",
        "\n",
        "For Summarization, we are using the `google/flan-t5-small` model as the base.\n",
        "This model works on a T4 compute and uses GPU power for training.\n",
        "\n",
        "We used the `google/flan-t5-small` model for several reasons\n",
        "1. This model is a fine-tuned version of T5 (Text-to-Text Transfer Transformer)\n",
        "2. It is pretrained for summarization usecases, which means it performs better than a regular T5 model and requires less fine-tuning.\n",
        "3. Flan model learns summarization more efficiently because it is built for Seq2Seq tasks\n",
        "4. It is light weight with 80 mn parameters, requiring less GPU, fast training and fast inferencing. Best balance of speed, memory usage, and accuracy.\n",
        "\n",
        "\n",
        "## Semantic Search\n",
        "For Semantic Search capabilities, we are training the `sentence-transformers/all-MiniLM-L6-v2` model by providing a training dataset of 2 similar papers as input. And then computing the similarity score during the training phase. The model uses this training set to better understand how similarity will work for research and academic papers.\n",
        "\n",
        "We used `sentence-transformers/all-MiniLM-L6-v2` model for several reasons -\n",
        "1. Unlike the general transformers like BERT, sentence-transformers models are pretrained for sentence similarity tasks.\n",
        "2. 22 mn parameters - making it light-weight, optimizing for speed and efficiency.\n",
        "3. Since academic papers use complex terminology, we need a model that already understands long-form text similarity.\n",
        "4. This model can encode text into dense vectors, meaning it works natively with vector search libraries like FAISS, Pinecone, and Elasticsearch.\n",
        "\n",
        "## Methodology\n",
        "\n",
        "1. The models are trained on academic data to fine-tune them further for the specific usecase of applying semantic search on academic papers.\n",
        "2. The performance of the fine-tuned models is compared with the base models to ensure the training was successful.\n",
        "3. The new model data is pushed to a GitHub repo and the application uses these models natively in the code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9B4BwdZeq8t"
      },
      "source": [
        "## Step 1 - Install and Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "NrSAM4kmZF-T",
        "outputId": "14e98789-b66b-4f6f-aae0-813fe7bf517a"
      },
      "outputs": [],
      "source": [
        "!pip install accelerate evaluate datasets faiss-cpu pandas rouge_score sentence-transformers tensorboard torch transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jiT1qRlzIPWg"
      },
      "outputs": [],
      "source": [
        "# Standard Library Imports (Built-in Python Modules)\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import logging\n",
        "import random\n",
        "import re\n",
        "\n",
        "# Third-Party Libraries (External Dependencies)\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import faiss\n",
        "\n",
        "# Google Colab Utilities\n",
        "from google.colab import drive\n",
        "\n",
        "# Hugging Face Libraries (Datasets, Transformers, Trainer)\n",
        "from datasets import load_dataset, DatasetDict\n",
        "from transformers import (\n",
        "    T5Tokenizer,\n",
        "    T5ForConditionalGeneration,\n",
        "    Trainer,\n",
        "    DataCollatorForSeq2Seq,\n",
        "    TrainingArguments\n",
        ")\n",
        "\n",
        "# Sentence Transformers (Semantic Search)\n",
        "from sentence_transformers import SentenceTransformer, losses, InputExample\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gktjV7enfCIn"
      },
      "source": [
        "## Step 2 - Environment Setup\n",
        "\n",
        "1. Check GPU availability\n",
        "2. Load the search and summarization models\n",
        "3. Load the dataset for both models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Qd_j8d8aHg7"
      },
      "outputs": [],
      "source": [
        "# confirm GPU is available.\n",
        "print(torch.cuda.is_available())\n",
        "print(torch.cuda.get_device_name(0))\n",
        "\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "# load pre-trained SBERT model optimized for academic papers\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "search_model=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "sbert_model = SentenceTransformer(search_model, device=device)\n",
        "\n",
        "# load transformer model and tokenizer\n",
        "summarization_model = \"google/flan-t5-small\"\n",
        "# The tokenizer is responsible for converting human-readable text into numerical representations (tokens) that a model can understand.\n",
        "tokenizer = T5Tokenizer.from_pretrained(summarization_model)\n",
        "t5_model = T5ForConditionalGeneration.from_pretrained(summarization_model).to(device)\n",
        "\n",
        "\n",
        "# load academic papers dataset\n",
        "dataset = load_dataset(\"scientific_papers\", \"arxiv\")\n",
        "print(\"Sample keys:\", dataset.keys())\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LyLTV2Z86SpO"
      },
      "source": [
        "# **Training the Summarization Model**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3mj1ZNiFLTv"
      },
      "source": [
        "## Step 1 - Preprocessing the Dataset (summarization model)\n",
        "\n",
        "1. Select a subset of the dataset for training and testing.\n",
        "2. A preprocess function that splits samples for training into input, labels and attention_mask for summarization model training.\n",
        "3. Initialize the tokenized_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sAG36paBFxxU"
      },
      "outputs": [],
      "source": [
        "training_size = 2000  # adjust as needed\n",
        "validation_size = 500\n",
        "test_size = 500\n",
        "\n",
        "# select small subsets from train, validation, and test\n",
        "small_train_dataset = dataset[\"train\"].select(range(min(training_size, len(dataset[\"train\"]))))\n",
        "small_validation_dataset = dataset[\"validation\"].select(range(min(validation_size, len(dataset[\"validation\"]))))\n",
        "small_test_dataset = dataset[\"test\"].select(range(min(test_size, len(dataset[\"test\"]))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zx1KKjJiEpLd"
      },
      "outputs": [],
      "source": [
        "# add labels to training dataset and clean labels for negatives.\n",
        "def preprocess_function(samples):\n",
        "    inputs = tokenizer([\"summarize: \" + doc for doc in samples[\"article\"]],\n",
        "                        max_length=256, truncation=True, padding=\"max_length\")\n",
        "    labels = tokenizer(samples[\"abstract\"],\n",
        "                        max_length=64, truncation=True, padding=\"max_length\")\n",
        "\n",
        "    # Convert PAD tokens to -100 for loss computation\n",
        "    labels[\"input_ids\"] = np.where(\n",
        "        np.array(labels[\"input_ids\"]) == tokenizer.pad_token_id, -100, np.array(labels[\"input_ids\"])\n",
        "    ).tolist()\n",
        "\n",
        "\n",
        "    return {\n",
        "        \"input_ids\": inputs[\"input_ids\"],\n",
        "        # attention_mask marks real tokens as 1 and padding tokens to ignore as 0. This helps the model ignore any padding tokens.\n",
        "        \"attention_mask\": inputs[\"attention_mask\"],\n",
        "        \"labels\": labels[\"input_ids\"]\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yh5JjVJcFdX4"
      },
      "outputs": [],
      "source": [
        "# Tokenize datasets\n",
        "tokenized_dataset = {\n",
        "    \"train\": small_train_dataset.map(preprocess_function, batched=True, remove_columns=[\"article\", \"abstract\", \"section_names\"]),\n",
        "    \"validation\": small_validation_dataset.map(preprocess_function, batched=True, remove_columns=[\"article\", \"abstract\", \"section_names\"]),\n",
        "    \"test\": small_test_dataset.map(preprocess_function, batched=True, remove_columns=[\"article\", \"abstract\", \"section_names\"]),\n",
        "}\n",
        "\n",
        "# Convert to DatasetDict format\n",
        "tokenized_dataset = DatasetDict(tokenized_dataset)\n",
        "\n",
        "# Set format for PyTorch training\n",
        "tokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "\n",
        "# Print dataset sizes\n",
        "print(f\"Train size: {len(tokenized_dataset['train'])}\")\n",
        "print(f\"Validation size: {len(tokenized_dataset['validation'])}\")\n",
        "print(f\"Test size: {len(tokenized_dataset['test'])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRyxr8xW3Tyx"
      },
      "source": [
        "## Step 2 - Train the Summarization Model\n",
        "1. Use a custom trainer to log runs and metrics for training loss, GPU consumption, validation loss and learning rate.\n",
        "2. Train the summarization model\n",
        "\n",
        "### Goal\n",
        "\n",
        "> Training and Validation loss should reduce with every epoch.\n",
        "> Loss should be lesser than or near 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EqWn1u7qGmZA"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./model_checkpoints/summarization_model\",\n",
        "    eval_strategy=\"epoch\",  # Validate after each epoch\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    logging_strategy=\"steps\",  # ✅ Log training loss regularly\n",
        "    logging_steps=50,  # ✅ Log every 50 steps\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    gradient_accumulation_steps=4,\n",
        "    num_train_epochs=10,\n",
        "    weight_decay=0.01,\n",
        "    max_grad_norm=1.0,\n",
        "    fp16=False,\n",
        "    save_total_limit=2,\n",
        "    report_to=\"tensorboard\",\n",
        ")\n",
        "\n",
        "# Data collator for batch padding\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=t5_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mCJBCC2ovIvZ"
      },
      "outputs": [],
      "source": [
        "# ✅ Create a SummaryWriter instance to store logs\n",
        "writer = SummaryWriter(\"./model_checkpoints/runs/summarization_model_training\")\n",
        "\n",
        "class CustomTrainer(Trainer):\n",
        "    def log(self, logs, start_time=None):\n",
        "        super().log(logs)\n",
        "\n",
        "        # ✅ Log additional metrics\n",
        "        if \"loss\" in logs:\n",
        "            writer.add_scalar(\"Training Loss\", logs[\"loss\"], logs.get(\"step\", 0))\n",
        "        if \"eval_loss\" in logs:\n",
        "            writer.add_scalar(\"Validation Loss\", logs[\"eval_loss\"], logs.get(\"step\", 0))\n",
        "        if \"learning_rate\" in logs:\n",
        "            writer.add_scalar(\"Learning Rate\", logs[\"learning_rate\"], logs.get(\"step\", 0))\n",
        "        writer.add_scalar(\"GPU Memory (MB)\", torch.cuda.memory_allocated() / (1024 * 1024), logs.get(\"step\", 0))\n",
        "\n",
        "        # ✅ Save logs in real-time\n",
        "        writer.flush()\n",
        "\n",
        "trainer = CustomTrainer(\n",
        "    model=t5_model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset[\"train\"],  # ✅ Use the small train dataset\n",
        "    eval_dataset=tokenized_dataset[\"validation\"],  # ✅ Use the small validation dataset\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LUg0JZRSG7fG"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsHxkAs429TG"
      },
      "source": [
        "## Step 3 - Evaluate the trained summarization model\n",
        "\n",
        "Test if the model summarizes information well by using the testing dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uhAW8juJJjfa"
      },
      "outputs": [],
      "source": [
        "trainer.evaluate(tokenized_dataset[\"test\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2BUZWOFiLsBd"
      },
      "outputs": [],
      "source": [
        "def generate_summary(text):\n",
        "    inputs = tokenizer(\"summarize: \" + text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
        "    inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}  # Move to GPU if available\n",
        "\n",
        "    summary_ids = t5_model.generate(\n",
        "        **inputs,\n",
        "        max_length=256,\n",
        "        min_length=100,\n",
        "        num_beams=5,  # use beam search to improve quality\n",
        "        repetition_penalty=2.0,  # discourages repeated phrases\n",
        "        length_penalty=1.0,  # adjusts summary length dynamically\n",
        "        early_stopping=True  # stops generatoin when a good summary is found\n",
        "    )\n",
        "\n",
        "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "# test with a sample\n",
        "sample_text = dataset[\"test\"][9][\"article\"]\n",
        "print(\"\\nOriginal Article:\", sample_text[:500])  # Print first 500 chars\n",
        "print(\"\\nGenerated Summary:\", generate_summary(sample_text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pb97-EbW3KLZ"
      },
      "source": [
        "## Step 4 - Save the fine tuned summarization model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S2oOBY4eOrID"
      },
      "outputs": [],
      "source": [
        "t5_model.save_pretrained(\"./model_checkpoints/fine_tuned_summarization_model\")\n",
        "tokenizer.save_pretrained(\"./model_checkpoints/fine_tuned_summarization_model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqI9UCKy6xbg"
      },
      "source": [
        "# **Training the Search Model**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NE3T3iGi3M0e"
      },
      "source": [
        "## Step 1 - Preprocessing for Search Model\n",
        "\n",
        "1. Fetch arxiv metadata from Google Drive\n",
        "2. Truncate the data to use a subset - upto 10000 records\n",
        "3. Use truncated data in a dataframe\n",
        "4. Clean the text by removing new lines, extraspcaes and special characters\n",
        "5. Prepare training dataset by forming a pair of 2 similar academic papers that belong to the same category.\n",
        "6. Manually check if the pairs are infact similar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "naiFWa5F5W3b"
      },
      "outputs": [],
      "source": [
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "file_path = \"/content/drive/MyDrive/Project_Talina_Manjula_Mar_2025/arxiv-metadata-oai-snapshot.json\"\n",
        "output_file = \"./model_checkpoints/truncated_arxiv.json\"\n",
        "\n",
        "N = 20000  # Change this to the number of records you want\n",
        "\n",
        "with open(file_path, \"r\") as infile, open(output_file, \"w\") as outfile:\n",
        "    for i, line in enumerate(infile):\n",
        "        if i >= N:\n",
        "            break  # Stop after N lines\n",
        "        outfile.write(line)\n",
        "\n",
        "print(f\"Truncated file saved as {output_file} with {N} records!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JXPM3IWn3nKs"
      },
      "outputs": [],
      "source": [
        "df = pd.read_json(\"./model_checkpoints/truncated_arxiv.json\", lines=True)\n",
        "\n",
        "\n",
        "# view dataset structure\n",
        "print(df.head())\n",
        "print(df.columns)\n",
        "print(df.info())\n",
        "\n",
        "df = df[[\"title\", \"abstract\", \"categories\"]].dropna()\n",
        "\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WaUKk6ma57MC"
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "    text = re.sub(r\"\\s+\", \" \", text)  # remove extra spaces\n",
        "    text = re.sub(r\"\\n\", \" \", text)  # remove newlines\n",
        "    text = re.sub(r\"[^a-zA-Z0-9,.?!:;()\\-]\", \" \", text)  # remove non-text characters\n",
        "    return text.strip()\n",
        "\n",
        "# apply cleaning\n",
        "df[\"title\"] = df[\"title\"].apply(clean_text)\n",
        "df[\"abstract\"] = df[\"abstract\"].apply(clean_text)\n",
        "\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7IlrxpZk59Lw"
      },
      "outputs": [],
      "source": [
        "# form pair papers with the exact same category\n",
        "train_examples = []\n",
        "grouped_papers = df.groupby(\"categories\")\n",
        "\n",
        "print(\"Unique Categories in Dataset:\")\n",
        "print(df[\"categories\"].unique())\n",
        "\n",
        "for _, group in grouped_papers:\n",
        "    papers = group.sample(n=min(200, len(group)), random_state=42)\n",
        "    for i in range(len(papers) - 1):\n",
        "        train_examples.append(InputExample(texts=[papers.iloc[i][\"title\"] + \" \" + papers.iloc[i][\"abstract\"],\n",
        "                                                  papers.iloc[i + 1][\"title\"] + \" \" + papers.iloc[i + 1][\"abstract\"]]))\n",
        "\n",
        "# shuffle data\n",
        "random.shuffle(train_examples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PUYCvpM75-_V"
      },
      "outputs": [],
      "source": [
        "# check 5 random training pairs\n",
        "for i in range(5):\n",
        "    text1, text2 = train_examples[i].texts\n",
        "    print(f\"**Pair {i+1}**\")\n",
        "    print(f\"**Text 1:** {text1[:300]}...\")  # Print first 300 characters\n",
        "    print(f\"**Text 2:** {text2[:300]}...\")\n",
        "    print(\"=\" * 100)\n",
        "\n",
        "print(len(train_examples))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mbN8v319a18"
      },
      "source": [
        "## Step 2 - Train the Search Model\n",
        "\n",
        "### Goal\n",
        "\n",
        "> The Validation Loss should be in the range of 1-2.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XXF4655x6BPh"
      },
      "outputs": [],
      "source": [
        "# convert to SBERT input format\n",
        "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=20)\n",
        "\n",
        "# loss function: maximizes similarity of paired texts\n",
        "train_loss = losses.MultipleNegativesRankingLoss(sbert_model)\n",
        "\n",
        "# Train SBERT model for 8 epochs\n",
        "sbert_model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=10, warmup_steps=100)\n",
        "\n",
        "# Save trained model\n",
        "sbert_model.save(\"./model_checkpoints/fine_tuned_search_model\")\n",
        "\n",
        "print(\"SBERT Training Complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hkEbmrSq6ErL"
      },
      "outputs": [],
      "source": [
        "# Load fine-tuned SBERT model\n",
        "model = SentenceTransformer(\"./model_checkpoints/fine_tuned_search_model\")\n",
        "# Encode all research papers into vectors\n",
        "paper_texts = df[\"title\"] + \" \" + df[\"abstract\"]\n",
        "paper_embeddings = model.encode(paper_texts.tolist())\n",
        "\n",
        "# Create FAISS index\n",
        "index = faiss.IndexFlatL2(paper_embeddings.shape[1])\n",
        "index.add(np.array(paper_embeddings))\n",
        "\n",
        "# Save FAISS index\n",
        "faiss.write_index(index, \"./model_checkpoints/paper_index.faiss\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A_xqX5BQ6JNb"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "\n",
        "# Load fine-tuned and baseline models\n",
        "fine_tuned_model = SentenceTransformer(\"./model_checkpoints/fine_tuned_search_model\")  # Your fine-tuned model\n",
        "baseline_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")  # Pretrained baseline\n",
        "\n",
        "# Sample text for testing\n",
        "sample_text = \"Artificial Intelligence in Healthcare\"\n",
        "\n",
        "# Generate embeddings\n",
        "embedding_finetuned = fine_tuned_model.encode(sample_text)\n",
        "embedding_baseline = baseline_model.encode(sample_text)\n",
        "\n",
        "# Compare embeddings\n",
        "similarity = np.dot(embedding_finetuned, embedding_baseline) / (np.linalg.norm(embedding_finetuned) * np.linalg.norm(embedding_baseline))\n",
        "\n",
        "print(f\"Cosine Similarity Between Fine-tuned and Baseline Model: {similarity:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9w49-D4d6LES"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Load models\n",
        "fine_tuned_model = SentenceTransformer(\"./model_checkpoints/fine_tuned_search_model\")\n",
        "baseline_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "# Get model parameters\n",
        "fine_tuned_params = list(fine_tuned_model.parameters())\n",
        "baseline_params = list(baseline_model.parameters())\n",
        "\n",
        "# Check if parameters changed\n",
        "param_changes = [torch.sum(fine_tuned_params[i] - baseline_params[i]).item() for i in range(len(fine_tuned_params))]\n",
        "\n",
        "print(f\"Total Parameter Change: {sum(param_changes):.6f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tXBVyh-_6MxH"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pandas as pd\n",
        "\n",
        "# Load fine-tuned and baseline models\n",
        "fine_tuned_model = SentenceTransformer(\"./model_checkpoints/fine_tuned_search_model\")  # Your fine-tuned model\n",
        "baseline_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")  # Baseline model\n",
        "\n",
        "# Load dataset (Make sure it's the same one used for fine-tuning)\n",
        "df = pd.read_json(\"./model_checkpoints/truncated_arxiv.json\", lines=True)\n",
        "df = df[[\"title\", \"abstract\"]].dropna()  # Keep only relevant columns\n",
        "\n",
        "# Search Query\n",
        "query = \"Give me some papers that talk about Albert Einstein and Theory of Relativity\"\n",
        "query_embedding_finetuned = fine_tuned_model.encode([query])\n",
        "query_embedding_baseline = baseline_model.encode([query])\n",
        "\n",
        "# Encode all research paper titles using both models\n",
        "paper_embeddings_finetuned = fine_tuned_model.encode(df[\"title\"].tolist())\n",
        "paper_embeddings_baseline = baseline_model.encode(df[\"title\"].tolist())\n",
        "\n",
        "# Compute similarity scores\n",
        "similarity_scores_finetuned = cosine_similarity(query_embedding_finetuned, paper_embeddings_finetuned)[0]\n",
        "similarity_scores_baseline = cosine_similarity(query_embedding_baseline, paper_embeddings_baseline)[0]\n",
        "\n",
        "# Get top 5 most similar papers for both models\n",
        "top_indices_finetuned = similarity_scores_finetuned.argsort()[-5:][::-1]\n",
        "top_indices_baseline = similarity_scores_baseline.argsort()[-5:][::-1]\n",
        "\n",
        "# Get top scores\n",
        "top_scores_finetuned = similarity_scores_finetuned[top_indices_finetuned]\n",
        "top_scores_baseline = similarity_scores_baseline[top_indices_baseline]\n",
        "\n",
        "# Print Results for Fine-tuned Model\n",
        "print(\"\\n**Fine-Tuned Model Results**:\")\n",
        "for i, (index, score) in enumerate(zip(top_indices_finetuned, top_scores_finetuned)):\n",
        "    row = df.iloc[index]\n",
        "    print(f\"**Rank {i+1}** (Similarity: {score:.4f})\")\n",
        "    print(f\"**Title:** {row['title']}\")\n",
        "    print(f\"**Abstract:** {row['abstract'][:300]}...\")\n",
        "    print(\"=\" * 100)\n",
        "\n",
        "# Print Results for Baseline Model\n",
        "print(\"\\n**Baseline Model Results**:\")\n",
        "for i, (index, score) in enumerate(zip(top_indices_baseline, top_scores_baseline)):\n",
        "    row = df.iloc[index]\n",
        "    print(f\"**Rank {i+1}** (Similarity: {score:.4f})\")\n",
        "    print(f\"**Title:** {row['title']}\")\n",
        "    print(f\"**Abstract:** {row['abstract'][:300]}...\")\n",
        "    print(\"=\" * 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjrZVgXSBNrP"
      },
      "source": [
        "# Download the Full Model Checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NKW0vRL4BOPz"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U3G_TJCGBPnh"
      },
      "outputs": [],
      "source": [
        "!cp -r /content/model_checkpoints /content/drive/MyDrive/"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
